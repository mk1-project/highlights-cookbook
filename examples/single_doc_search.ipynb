{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Search for Large Single Document with Highlights\n",
    "\n",
    "This notebook demonstrates a streamlined approach to document question-answering using Highlights:\n",
    "\n",
    "1. **Efficiency**: Processes large documents without sending the entire content to a frontier LLM\n",
    "2. **Precision**: Uses Highlights' contextual awareness to identify relevant passages\n",
    "3. **Cost-effectiveness**: Reduces token usage by focusing only on relevant sections\n",
    "\n",
    "The implementation follows these steps:\n",
    "1. Convert a PDF document to text\n",
    "2. Split the text into page-level chunks\n",
    "3. Use Highlights API to retrieve contextually relevant chunks\n",
    "4. Send only the most relevant chunks to OpenAI for response generation\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2 openai python-dotenv\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "from base_client import HighlightsClient\n",
    "from utils import PDFProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Environment Variables\n",
    "\n",
    "Create a .env file with your API keys:\n",
    "```\n",
    "HIGHLIGHTS_API_KEY=your-highlights-api-key\n",
    "OPENAI_API_KEY=your-openai-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "highlights_client = HighlightsClient(api_key=os.getenv('HIGHLIGHTS_API_KEY'))\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Processing\n",
    "\n",
    "Extracting text from PDF documents presents several challenges including preserving formatting and structure. The PDFProcessor class handles this complexity while maintaining appropriate chunk boundaries for optimal retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = PDFProcessor(highlights_client)\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = 'data/border_act.pdf'\n",
    "\n",
    "# Extract text chunks from PDF\n",
    "text_chunks = processor.extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(f\"Extracted {len(text_chunks)} pages from PDF\")\n",
    "print(\"\\nSample from first page:\")\n",
    "print(text_chunks[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Retrieval with Highlights\n",
    "\n",
    "Unlike vector search that treats chunks in isolation, Highlights evaluates each segment within its surrounding context, significantly improving retrieval relevance for large, complex documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Am I an eligible individual for CONDITIONAL PERMANENT RESIDENT STATUS? I was paroled into the us in 2020.\"\n",
    "\n",
    "# Search for relevant chunks\n",
    "relevant_chunks = processor.search_relevant_chunks(\n",
    "    query=query,\n",
    "    text_chunks=text_chunks,\n",
    "    top_n=5  # Limiting to top 5 chunks balances completeness with token efficiency\n",
    ")\n",
    "\n",
    "print(f\"Found {len(relevant_chunks)} relevant chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation\n",
    "\n",
    "By forwarding only the most relevant chunks to a frontier model, we achieve two key benefits:\n",
    "1. Reduced token consumption and lower costs\n",
    "2. Higher quality responses by eliminating distracting or irrelevant content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response using OpenAI\n",
    "response = processor.generate_response(\n",
    "    query=query,\n",
    "    context=relevant_chunks\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
