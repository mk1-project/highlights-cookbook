{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Search for Large Single Document with Highlights\n",
    "\n",
    "This notebook demonstrates a streamlined approach to document search using Highlights:\n",
    "\n",
    "1. **Efficiency**: Processes large documents without sending the entire content to a frontier LLM\n",
    "2. **Precision**: Uses Highlights' contextual awareness to identify relevant passages\n",
    "3. **Cost-effectiveness**: Reduces token usage by focusing only on relevant sections\n",
    "\n",
    "The implementation follows these steps:\n",
    "1. Convert a PDF document to text\n",
    "2. Split the text into page-level chunks\n",
    "3. Use Highlights API to retrieve contextually relevant chunks\n",
    "4. Send only the most relevant chunks to OpenAI for response generation\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai python-dotenv tiktoken PyPDF2\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from base_client import HighlightsClient\n",
    "from utils import PDFProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Environment Variables\n",
    "\n",
    "Create a .env file with your API keys:\n",
    "```\n",
    "HIGHLIGHTS_API_KEY=your-highlights-api-key\n",
    "OPENAI_API_KEY=your-openai-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "highlights_client = HighlightsClient(api_key=os.getenv('HIGHLIGHTS_API_KEY'))\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Processing\n",
    "\n",
    "For this example we use the 2023 10K filling for UBER with about 169K tokens, which is too large to be directly loaded into GPT4o or Sonnet. We extract the text from the pdf and use a simple chunking at the section level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 191 pages from PDF with a total of 169419 tokens\n"
     ]
    }
   ],
   "source": [
    "# Initialize processor\n",
    "processor = PDFProcessor(highlights_client)\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = 'data/uber_2023_10k.pdf'\n",
    "\n",
    "# Extract text chunks from PDF\n",
    "text_chunks = processor.extract_text_from_pdf(pdf_path)\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"o200k_base\") # gpt-4o tokenizer\n",
    "num_tokens = len(encoding.encode(\"\".join(text_chunks)))\n",
    "\n",
    "print(f\"Extracted {len(text_chunks)} pages from PDF with a total of {num_tokens} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Retrieval with Highlights\n",
    "\n",
    "Unlike vector search that treats chunks in isolation, Highlights evaluates each segment within its surrounding context. This can lead to improvements in retrieval accuracy for complex documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 relevant chunks\n"
     ]
    }
   ],
   "source": [
    "query = \"What drove revenue change for UBER in FY23?\"\n",
    "\n",
    "# Search for relevant chunks\n",
    "relevant_chunks = processor.search_relevant_chunks(\n",
    "    query=query,\n",
    "    text_chunks=text_chunks,\n",
    "    top_n=5  # Limiting to top 5 chunks balances completeness with token efficiency\n",
    ")\n",
    "\n",
    "print(f\"Found {len(relevant_chunks)} relevant chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation\n",
    "\n",
    "By forwarding only the most relevant chunks to a frontier model, we achieve two key benefits:\n",
    "1. Reduced token consumption and lower costs\n",
    "2. Higher quality responses by eliminating distracting or irrelevant content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "In FY23, Uber's revenue change was primarily driven by several key factors:\n",
      "\n",
      "1. **Increase in Mobility Revenue**: Mobility revenue increased by $5.8 billion, or 41%, largely due to a 31% year-over-year increase in Mobility Gross Bookings, which was driven by an increase in trip volumes.\n",
      "\n",
      "2. **Growth in Delivery Revenue**: Delivery revenue rose by $1.3 billion, or 12%, primarily attributed to a 14% increase in Delivery Gross Bookings, which was driven by higher delivery orders and larger basket sizes.\n",
      "\n",
      "3. **Decrease in Freight Revenue**: Revenue growth was partially offset by a $1.7 billion decrease in the Freight segment, with Freight Gross Bookings declining 25% year-over-year due to lower revenue per load and volume amid a challenging freight market cycle.\n",
      "\n",
      "4. **Business Model Changes**: The overall increases in Mobility and Delivery revenue were also negatively impacted by business model changes in some countries, which classified certain sales and marketing costs as contra revenue. This negatively affected revenue by $368 million in Mobility and $796 million in Delivery.\n",
      "\n",
      "Overall, the combination of increased gross bookings in Mobility and Delivery, alongside the decline in Freight, drove the change in Uber's revenue for FY23.\n"
     ]
    }
   ],
   "source": [
    "# Generate response using OpenAI\n",
    "response = processor.generate_response(\n",
    "    query=query,\n",
    "    context=relevant_chunks\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
