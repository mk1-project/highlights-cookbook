{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Search Across Multiple Documents with Highlights\n",
    "\n",
    "This notebook demonstrates a streamlined approach to multi-document question-answering using Highlights:\n",
    "\n",
    "1. **Scalability**: Processes collections of documents up to 2M tokens without context window limitations\n",
    "2. **Precision**: Uses Highlights' contextual awareness to identify relevant passages across document sections\n",
    "3. **Efficiency**: Optimizes both computational resources and token usage by focusing only on relevant sections\n",
    "\n",
    "The implementation follows these steps:\n",
    "1. Prepare and chunk multiple documents with appropriate metadata\n",
    "2. Use Highlights API to retrieve contextually relevant chunks across all documents\n",
    "3. Send only the most relevant chunks to OpenAI for response generation\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai python-dotenv datasets langchain_text_splitters\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "from base_client import HighlightsClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Environment Variables\n",
    "\n",
    "Create a .env file with your API keys:\n",
    "```\n",
    "HIGHLIGHTS_API_KEY=your-highlights-api-key\n",
    "OPENAI_API_KEY=your-openai-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "highlights_client = HighlightsClient(api_key=os.getenv('HIGHLIGHTS_API_KEY'))\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Collection Processing\n",
    "\n",
    "Multi-document search introduces new challenges around chunking and metadata preservation. The approach below handles these challenges by maintaining document identity while maintaining contextually-aware retrieval across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rdoshi/repos/highlights-cookbook/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|██████████| 77932/77932 [00:00<00:00, 80239.69 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 chapters from textbooks by Suza and Lamkey\n"
     ]
    }
   ],
   "source": [
    "# Load document collection\n",
    "from datasets import load_dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Extract textbooks from specific authors as an example collection\n",
    "ds = load_dataset(\"princeton-nlp/TextbookChapters\")\n",
    "documents = {\n",
    "    item['path']: item['chapter']\n",
    "    for item in ds['train']\n",
    "    if \"Suza_and_Lamkey\" in item['path']\n",
    "}\n",
    "\n",
    "# Print summary of loaded documents\n",
    "print(f\"Loaded {len(documents)} chapters from textbooks by Suza and Lamkey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Chunking with Metadata Preservation\n",
    "\n",
    "The approach below uses recursive splitting with chunk sizes (between 1000-10000 characters) and preserves document metadata in an XML-like format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 26 chapters into 118 chunks\n"
     ]
    }
   ],
   "source": [
    "# Define an xml-style template for each chunk with text and metadata\n",
    "text_chunk_template = \"\"\"\n",
    "<document>\n",
    "    <metadata>\n",
    "        <name>{document_name}</name>\n",
    "        <chunk_id>{document_chunk_id}</chunk_id>\n",
    "    </metadata>\n",
    "    <content>{chapter_text}</content>\n",
    "</document>\n",
    "\"\"\"\n",
    "\n",
    "# Use langchain's recursive text splitter as an example\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=8192,  # Balance context depth and retrieval granularity\n",
    "    chunk_overlap=0,  # No overlap for this example, though 10-20% can improve context coherence\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \"]\n",
    ")\n",
    "\n",
    "text_chunks = []\n",
    "for path, chapter in documents.items():\n",
    "    chunks = text_splitter.split_text(chapter)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        document_text_chunk = text_chunk_template.format(\n",
    "            document_name=path,\n",
    "            chapter_text=chunk,\n",
    "            document_chunk_id=i\n",
    "        )\n",
    "        text_chunks.append(document_text_chunk)\n",
    "\n",
    "print(f\"Split {len(documents)} chapters into {len(text_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Document Retrieval with Highlights\n",
    "\n",
    "Unlike traditional vector search that evaluates chunks in isolation, Highlights analyzes each segment within its broader context. This approach significantly improves retrieval quality across diverse document collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 relevant passages from across 26 documents\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the different ways in which mutations can be classified?\"\n",
    "\n",
    "# Search for relevant chunks across all documents\n",
    "highlights_response = highlights_client.search(\n",
    "    query=query,\n",
    "    chunk_txts=text_chunks,\n",
    "    top_n=5,\n",
    ")\n",
    "\n",
    "relevant_passages = [result['chunk_txt'] for result in highlights_response['results']]\n",
    "print(f\"Retrieved {len(relevant_passages)} relevant passages from across {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation with Contextual Awareness\n",
    "\n",
    "By forwarding only the most relevant chunks to a frontier model, we achieve three key benefits:\n",
    "1. Extended document coverage beyond single-document context windows\n",
    "2. Reduced token consumption with corresponding cost savings\n",
    "3. Higher quality responses by eliminating cross-document noise and irrelevant content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "Mutations can be classified in several ways, including:\n",
      "\n",
      "1. **Causal Agent**: This classification distinguishes between spontaneous mutations, which occur naturally without intentional exposure to a mutagen, and induced mutations, which are caused by mutagens such as chemicals or radiation.\n",
      "\n",
      "2. **Rate or Frequency of Occurrence**:\n",
      "   - **Rare Mutations**: These occur infrequently in populations and are usually recessive, often hidden in heterozygotes.\n",
      "   - **Recurrent Mutations**: These occur repeatedly and can influence gene frequency in populations.\n",
      "\n",
      "3. **Kind of Tissue Involved and Inheritance Type**:\n",
      "   - **Somatic Mutations**: Occur in somatic tissue and are not passed to offspring.\n",
      "   - **Germinal (or Germ-Line) Mutations**: Occur in reproductive cells and can be inherited by future generations.\n",
      "\n",
      "4. **Impact on Fitness or Function**:\n",
      "   - **Deleterious Mutations**: Harmful mutations that decrease the fitness of an individual.\n",
      "   - **Advantageous Mutations**: Beneficial mutations that increase the fitness of an individual.\n",
      "   - **Neutral Mutations**: Mutations that have no significant impact on fitness.\n",
      "   - **Lethal Mutations**: Mutations that result in the death of the organism.\n",
      "\n",
      "5. **Molecular Structure and Scale of the Mutation**: \n",
      "   - **Point Mutations**: Affect a single gene or base pair.\n",
      "   - **Chromosomal Mutations**: Affect the structure or number of chromosomes, including deletions, duplications, inversions, and translocations.\n",
      "\n",
      "These classifications help in understanding the nature of mutations and their implications in genetics and breeding.\n"
     ]
    }
   ],
   "source": [
    "# Generate response using OpenAI with the consolidated context\n",
    "context = '\\n\\n'.join(relevant_passages)\n",
    "combined_prompt = f\"\"\"\n",
    "Context information is below.\n",
    "----------------\n",
    "{context}\n",
    "----------------\n",
    "Using the above context, please answer the following question: {query}\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",  # or another appropriate model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": combined_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
