{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Search Across Multiple Documents with Highlights\n",
    "\n",
    "This notebook demonstrates a streamlined approach to multi-document question-answering using Highlights:\n",
    "\n",
    "1. **Scalability**: Processes collections of documents up to 2M tokens without context window limitations\n",
    "2. **Precision**: Uses Highlights' contextual awareness to identify relevant passages across document sections\n",
    "3. **Efficiency**: Optimizes both computational resources and token usage by focusing only on relevant sections\n",
    "\n",
    "The implementation follows these steps:\n",
    "1. Prepare and chunk multiple documents with appropriate metadata\n",
    "2. Use Highlights API to retrieve contextually relevant chunks across all documents\n",
    "3. Send only the most relevant chunks to OpenAI for response generation\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai python-dotenv datasets langchain_text_splitters\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "from base_client import HighlightsClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Environment Variables\n",
    "\n",
    "Create a .env file with your API keys:\n",
    "```\n",
    "HIGHLIGHTS_API_KEY=your-highlights-api-key\n",
    "OPENAI_API_KEY=your-openai-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "highlights_client = HighlightsClient(api_key=os.getenv('HIGHLIGHTS_API_KEY'))\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Collection Processing\n",
    "\n",
    "Multi-document search introduces new challenges around chunking and metadata preservation. The approach below handles these challenges by maintaining document identity while maintaining contextually-aware retrieval across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document collection\n",
    "from datasets import load_dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Extract textbooks from specific authors as an example collection\n",
    "ds = load_dataset(\"princeton-nlp/TextbookChapters\")\n",
    "documents = {\n",
    "    item['path']: item['chapter']\n",
    "    for item in ds['train']\n",
    "    if \"Suza_and_Lamkey\" in item['path']\n",
    "}\n",
    "\n",
    "# Print summary of loaded documents\n",
    "print(f\"Loaded {len(documents)} chapters from textbooks by Suza and Lamkey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Chunking with Metadata Preservation\n",
    "\n",
    "The approach below uses recursive splitting with chunk sizes (between 1000-10000 characters) and preserves document metadata in an XML-like format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an xml-style template for each chunk with text and metadata\n",
    "text_chunk_template = \"\"\"\n",
    "<document>\n",
    "    <metadata>\n",
    "        <name>{document_name}</name>\n",
    "        <chunk_id>{document_chunk_id}</chunk_id>\n",
    "    </metadata>\n",
    "    <content>{chapter_text}</content>\n",
    "</document>\n",
    "\"\"\"\n",
    "\n",
    "# Use langchain's recursive text splitter as an example\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=8192,  # Balance context depth and retrieval granularity\n",
    "    chunk_overlap=0,  # No overlap for this example, though 10-20% can improve context coherence\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \"]\n",
    ")\n",
    "\n",
    "text_chunks = []\n",
    "for path, chapter in documents.items():\n",
    "    chunks = text_splitter.split_text(chapter)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        document_text_chunk = text_chunk_template.format(\n",
    "            document_name=path, \n",
    "            chapter_text=chunk, \n",
    "            document_chunk_id=i\n",
    "        )\n",
    "        text_chunks.append(document_text_chunk)\n",
    "\n",
    "print(f\"Split {len(documents)} chapters into {len(text_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Document Retrieval with Highlights\n",
    "\n",
    "Unlike traditional vector search that evaluates chunks in isolation, Highlights analyzes each segment within its broader context. This approach significantly improves retrieval quality across diverse document collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the different ways in which mutations can be classified?\"\n",
    "\n",
    "# Search for relevant chunks across all documents\n",
    "highlights_response = highlights_client.search(\n",
    "    query=query,\n",
    "    chunk_txts=text_chunks,\n",
    "    top_n=5  # Limiting to top 5 chunks balances completeness with token efficiency\n",
    ")\n",
    "\n",
    "relevant_passages = [result['chunk_txt'] for result in highlights_response['results']]\n",
    "print(f\"Retrieved {len(relevant_passages)} relevant passages from across {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation with Contextual Awareness\n",
    "\n",
    "By forwarding only the most relevant chunks to a frontier model, we achieve three key benefits:\n",
    "1. Extended document coverage beyond single-document context windows\n",
    "2. Reduced token consumption with corresponding cost savings\n",
    "3. Higher quality responses by eliminating cross-document noise and irrelevant content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response using OpenAI with the consolidated context\n",
    "context = '\\n\\n'.join(relevant_passages)\n",
    "combined_prompt = f\"\"\"\n",
    "Context information is below.\n",
    "----------------\n",
    "{context}\n",
    "----------------\n",
    "Using the above context, please answer the following question: {query}\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",  # or another appropriate model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": combined_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
